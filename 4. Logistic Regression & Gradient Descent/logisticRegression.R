############### LOGISTIC REGRESSION ###############
#
# Logistic regression is a type of probabilistic binary classification model.  Whereas linear classification imposes a 
# hard threshold on a signal s, such as h(x) = sign(s), and linear regression uses the the raw signal, h(x) = s, 
# logistic regression falls in between these two in that it uses a non-linear transform to impose a soft threshold on 
# the signal,h(x) = theta(s).  The soft threshold bounds the output between two values, like in linear classification, 
# however, it is real valued as in linear regression.
#
# The non-linear transform used for logistic regression is sigmoid-shaped and bounds the output between 0 and 1.  It 
# is interpreted as a probability; very positive signals will produce an output close to 1, very negative signals will
# tend to be closer to -1, and a signal of 0 will correspond an output of 0.5.  The soft threshold allows the output 
# to reflect the level of uncertainty we have about classifying something.  The signal, s = t(w)%*%x, can be 
# interpreted as a risk score.
#
# The final hypothesis, g(x) = theta(t(w)%*%x) is a genuine probability which approximates a noisy target of the 
# following form: 
#
# P(y|x) = f(x) for y = +1; 1 - f(x) for y = -1
#
# where f(x): R^(d) -> [0,1] is the probability of a positive classification.  The responses in the training set consist of sample
# values of y that happen to be generated by f(x).  f(x) is the target that is learned here.
#
# The error measure for logistic regression is based on likelihood.  It measures how likely we are to generate y from
# x under the assumption that h = f; in other words, it measures the probability of the data given the hypothesis.
# Maximizing the likelihood translates into minimizing an error measure.  Unlike the error measure for linear
# regression, there is no closed form solution for the minimum of the error measure for logistic regression; thus, 
# an iterative solution is used instead.  
#
# In the case of logistic regression, the shape of E_in with respect to w is convex; thus, gradient descent can be
# used to find the global minimum without having to worry about local minima or false minima (flat segments).  
#
# The algorithm for logistic regression is as follows:
#   1. Initialize the weights at t = 0 to w(0) (This makes sense with logistic regression; initially we know nothing)
#   2. for t = 0, 1, 2, ... do
#   3.  Compute the gradient of E_in
#   4.  Update the weights: w(t+1) = w(t) - eta*gradient(E_in)
#   5.  Iterate to the next t until a stopping condition is met
#   6. Return the final weights w
#
# NOTE: Stochastic gradient descent was used for the following implementation of logistic regression; thus, in this 
# case, the algorithm looks as follows:
#
# The algorithm for logistic regression is as follows:
#   1. Initialize the weights at t = 0 to w(0) (This makes sense with logistic regression; initially we know nothing)
#   2. for t = 0, 1, 2, ... do
#   3.  for i = 1, 2, 3, ... N
#   4.    Choose a point (xn, yn) from the training set at random without replacement
#   5.    Compute the gradient of e_in(h(xi), yi)
#   6.    Update the weights: w = w - eta*gradient(e(h(xi), yi))
#   5.  Iterate to the next epoch t until a stopping condition is met
#   6. Return the final weights w
#
############### DEFINITIONS ###############
#
# f = The target function
# g = The final hypothesis
# w = The weights used by a given hypothesis to generate the signal
# E_in = In-sample error
# E_out = Out-of-sample error
# d = The dimensionality of the data set
# s = signal = SIGMA(i = 0, d) (w_i * x_i)
# eta = Learning rate used to adjust step size according to the steepness of the error surface during gradient descent
# theta = non-linear transform used by logistic regression
# Cross-entropy error = the cross-entropy between a hypothesis, h, and a realization of f, y.
# epoch = A full pass through all N data points during stochastic gradient descent.
#
# theta(s) = e^(s)/(1 + e^(s))
#
# E(w) = (1/N)*SIGMA(n = 1, N) ln(1 + e^(-yn*t(w)%*%xn))
#
# gradient(E_in) = (-1/N)*SIGMA(n = 1, N) (yn*xn)/(1 + e^(yn*t(w)%*%xn))
#
############### EXPERIMENT ###############
#
# This simulation implements logistic regression and uses to approximate the probabilities associated with the 
# binary classification of simulated data.  For this simulation, y is a deterministic function on x; thus, f is a
# 0/1 probability.  
#
# A target function f and a dataset D in d = 2 are created.  X = [-1, 1] x [-1, 1] with uniform probability of picking 
# each x in X.  Each run chooses a random line in the plane as the boundary between f(x) = 1 (where y has to be +1) and
# f(x) = 0 (where y has to be -1) by taking the line passing through two random, uniformly distributed points in 
# [-1, 1] x [-1, 1].  A specified number of training examples is generated and stochastic gradient descent is used to 
# find g.  Out-of-sample cross-entropy error is estimated by generating a sufficiently large separate set of points to 
# evaluate the error.  The experiment is repeated a specified number of times and the results are averaged.  
#
# The algorithm is stopped when ||w_(t-1) - w_(t)|| < 0.01, where w_(t) denotes the weight vector at the end of epoch t.
# This stopping condition takes advantage of the fact that a learning rate is being used in the stochastic gradient 
# descent algorithm; as w approaches a value that minimizes the error function, the change in w becomes progressively
# smaller from epoch to epoch because slope along the gradient is relatively flat. A random permutation of 1, 2, ..., N 
# is used to the present the data points to the algorithm within each epoch.
#
# Plots of the final hypothesis and corresponding "target function" against training and test data are produced using 
# the results from the final trial.  "Target function" is in quotations because what is plotted is the line representing
# the boundary between f(x) = 1 and f(x) = 0.  

############### IMPLEMENTATION ###############

## Function for generating the data and, if specified, a target function
data.generate <- function(N = 100, ext = 1, generateTarget = FALSE){
  # Generate the points
  x1 <- runif(N, -ext, ext)
  x2 <- runif(N, -ext, ext)
  if (!generateTarget)
    return(data.frame(x1, x2))
  # Draw a random line in the area (target function)
  point <- runif(2, -ext, ext)
  point2 <- runif(2, -ext, ext)
  slope <- (point2[2] - point[2]) / (point2[1] - point[1])
  intercept <- point[2] - slope * point[1]
  # Set up a factor for point classification
  y <- as.numeric(x1 * slope + intercept > x2) * 2 - 1
  # Return the values in a list
  data <- data.frame(x1,x2,y)
  return(list(data = data,slope = slope, intercept = intercept))
} 

## Performs stochastic gradient descent to determine the weights that minimize the error measure for logistic regression
## Returns a list containing the weight vector for the final hypothesis and the number of epochs it took to find it
## D - Training data
## w - Initial weight vector
## eta - Learning rate
## step_threshold - Stop condition: the minimum step size between the norm of the weight vector between two consecutive epochs
logisticRegression.SGD <- function(D, w, eta, step_threshold = 0.01) {
  epochs <- 0
  repeat {
    randomPermutation <- sample(1:nrow(D))  # create random permutation for current epoch
    w_0 <- w                                # store the weight vector from the previous epoch
    for(i in randomPermutation) {           # perform gradient descent on one example at a time
      x <- as.numeric(D[i, c(-length(D))])  # slice out the x vector
      y <- D[i, c(length(D))]               # slice out the response
      w <- w - eta*(-y*x)/(1 + exp(y*t(w)%*%x)) # update w: w = w - eta*gradient(e(h(xi), yi))
    }
    epochs <- epochs + 1                    # increment the number of epochs taken
    if(norm(as.matrix(w_0) - as.matrix(w), type = 'F') < step_threshold) # check the stopping condition
      break
  }
  list(w = w, numEpochs = epochs) 
}

## Performs logistic regression to learn a final hypothesis for the probabilities associated with the simulated data
## Approximates the out-of-sample cross entropy error
## Repeats the experiment a specified number of times and returns average E_out and average number of epochs in a list
## Plots a final hypothesis and corresponding "target function" against training and test data using results from the final trial
## N_train - Number of training examples
## N_test - Size of the test set
## numTrials - Number of times the experiment is repeated
## w_0 - Initial weight vector
## eta - learning rate
## step_threshold - stop condition to be passed to the stochastic gradient descent function
logisticRegression.simulate <- function(N_train = 100, N_test = 1000, numTrials = 100, w_0 = c(0,0,0), eta = 0.01, step_threshold = 0.01) {
  # Initialize vectors to hold the results from each trial
  E_out <- numeric(numTrials)
  numEpochs <- numeric(numTrials)
  for(i in 1:numTrials) {
    sim <- data.generate(N = N_train, generateTarget = TRUE) # generate training data and boundary
    D <- cbind(1, sim$data)                                  # extract the training set
    results <- logisticRegression.SGD(D, w_0, eta)           # perform stochastic gradient descent and store the results
    numEpochs[i] <- results$numEpochs                        # store the number of epochs taken by SGD
    w <- results$w                                           # store the weight vector determined by SGD
    D_test <- cbind(1, data.generate(N = N_test))            # generate a test set
    y <- as.numeric(D_test$x1 * sim$slope + sim$intercept > D_test$x2) * 2 - 1 # evaluate y on each of the test points
    D_test <- cbind(D_test, y)                               # Add the y column to the test set
    crossEntropy_error <- 0
    for(j in 1:nrow(D_test)) {                               # evaluate cross entropy error on each test point
      x <- as.numeric(D_test[j, c(-length(D_test))])
      y <- D_test[j, c(length(D_test))]
      crossEntropy_error <- crossEntropy_error + log(1 + exp(-y*t(w)%*%x))
    }
    crossEntropy_error <- crossEntropy_error/nrow(D_test)    # take the average
    E_out[i] <- crossEntropy_error                           # store the result
  }
  
  # Plot the points and f and g functions from the last trial (purely illustrative purposes)
  
  library(ggplot2)
  library(gridExtra)
  
  plot1 <- qplot(sim$data$x1, sim$data$x2, col= as.factor(sim$data$y), data = sim$data, xlab = 'x1', ylab = 'x2', main = 'Final Hypothesis and Target Function against Training Data') + 
    geom_abline(intercept = sim$intercept, slope = sim$slope) +
    geom_abline(intercept = -w[1]/w[3], slope = -w[2]/w[3], col=3) +
    theme(legend.title = element_blank())
  
  plot2 <- qplot(D_test$x1, D_test$x2, col= as.factor(D_test$y), data = D_test, xlab = 'x1', ylab = 'x2', main = 'Final Hypothesis and Target Function Against Test Data') + 
    geom_abline(intercept = sim$intercept, slope = sim$slope) +
    geom_abline(intercept = -w[1]/w[3], slope = -w[2]/w[3], col=3) +
    theme(legend.title = element_blank())
  
  grid.arrange(plot1, plot2, ncol=2)
  
  # return the average of the results obtained across the trials
  list(E_out = mean(E_out), avg_number_of_epochs = mean(numEpochs)) 
}

logisticRegression.simulate(numTrials = 1)     # single trial
# logisticRegression.simulate(numTrials = 10)    # more trials; longer running time
# logisticRegression.simulate(numTrials = 100) # Problems 8 & 9 ; much longer running time